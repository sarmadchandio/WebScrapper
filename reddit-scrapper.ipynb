{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9irb3UV6xUUm0mNJ5mUHY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarmadchandio/WebScrapper/blob/main/reddit-scrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install praw\n",
        "!pip install wordcloud matplotlib\n",
        "!pip install gensim\n",
        "!pip install bertopic"
      ],
      "metadata": {
        "id": "1ESXFe1VGkhp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc0a4AwtFtRo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import praw\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Big Data Pipeline\n",
        "\n",
        "![img](https://drive.google.com/uc?id=1I8hfMsmjKTIl76DxsA9F9tzngkhUo6dJ)\n"
      ],
      "metadata": {
        "id": "9pjbzbp-4PKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part1: Let's scrape some reddit data!"
      ],
      "metadata": {
        "id": "chfrY9R3JLu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Annoying things that we need to setup before starting.\n",
        "1. Create a Reddit account.\n",
        "2. Go to [this link](https://www.reddit.com/prefs/apps)\n",
        "3. Use the following image to setup things\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?id=1V62iD3KVlrPoyLRqbqtaRGcQRvJGh3i8\"\n",
        "width=\"700\"/>\n",
        "</div>\n",
        "\n",
        "- Enter http://localhost:8080 in the redirect uri\n",
        "- Copy personal_use_script and paste it in the personal_use_script below. </br>\n",
        "- Copy secret and paste it in the client_secret below\n",
        "\n",
        "</br>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The deal with *APIs* is simple. It is annoying to setup ONCE but it easy to use over and over and over and over ... </br>\n",
        "Think of an API as a waiter who takes order from you gets food from the kitchen.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "S5njkZWxGCB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "personal_use_script = ''\n",
        "client_secret = ''\n",
        "user_agent = 'Dont mind me'\n",
        "\n",
        "\n",
        "reddit = praw.Reddit(client_id=personal_use_script, client_secret=client_secret, user_agent=user_agent, check_for_async=False)\n",
        "print(\"We are done setting up the api!\")"
      ],
      "metadata": {
        "id": "pXhuCO28FyZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def get_reddit_posts(subreddit_name, limit=1000):\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    posts = []\n",
        "\n",
        "    for submission in subreddit.hot(limit=limit):  # change to .new, .top, or .controversial if needed\n",
        "      post_data = {\n",
        "          \"title\": submission.title,\n",
        "          \"score\": submission.score,\n",
        "          \"selftext\": submission.selftext,\n",
        "          # \"id\": submission.id,\n",
        "          # \"url\": submission.url,\n",
        "          # \"created\": submission.created\n",
        "          # ... any other attributes you are interested in\n",
        "      }\n",
        "      posts.append(post_data)\n",
        "\n",
        "    return posts\n"
      ],
      "metadata": {
        "id": "19YWQjYmA6Mn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try changing 'politics' to the subreddit of your liking!\n",
        "\n"
      ],
      "metadata": {
        "id": "hOO5E5O1cKR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the latest 1000 posts from r/politics\n",
        "posts = get_reddit_posts('politics', limit=1000)\n",
        "print(\"Posts collected successfully!\")"
      ],
      "metadata": {
        "id": "zN6nvLmnBFif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "df_posts = pd.DataFrame(posts)\n",
        "df_posts"
      ],
      "metadata": {
        "id": "JtAoaoreBSlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "file_path = '/content/gdrive/MyDrive/sample_data.csv'\n",
        "df_posts.to_csv(file_path, index=False)"
      ],
      "metadata": {
        "id": "ISSZ-ownWBxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: But what do I do with so much text?\n",
        "\n",
        "Before doing so much with big text let's try something small with just two sentences. </br>\n",
        "Don't tell my peers that I showed you how simple it is!"
      ],
      "metadata": {
        "id": "ZfKpGziYcVXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "sentence1 = 'This is a Sentence. This sentence will be lowerCASED, LEMMAtized, removed OF stopwords, and tokenized'\n",
        "sentence2 = 'I am another SENTENCE! I will be used to demonstrate how lemmatization, stopword removal, lowercasing, and tokenization work.'\n",
        "\n",
        "print(sentence1)\n",
        "print(sentence2)"
      ],
      "metadata": {
        "id": "TXMsp4QdW-2a",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick question: Is 'Toxic' the same as 'toxic' or 'toXic' or 'TOXIC'?"
      ],
      "metadata": {
        "id": "2P75d1nvdxdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "sentence1 = sentence1.lower()\n",
        "sentence2 = sentence2.lower()\n",
        "\n",
        "print(sentence1)\n",
        "print(sentence2)"
      ],
      "metadata": {
        "id": "hzmb-JiGdkU_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "quick question: Is 'stripped' the same as 'strip' or 'used' the same as 'use'?"
      ],
      "metadata": {
        "id": "Xx2IjAM_gTP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "sentence1 = sentence1.replace('cased', 'case').replace('lemmatized', 'lemma').replace('removed', 'remove').replace('tokenized', 'token').replace('stopwords', 'stopword').replace('lowercased', 'lowercase')\n",
        "sentence2 = sentence2.replace('used', 'use').replace('lemmatization', 'lemma').replace('removal', 'remove').replace('tokenization', 'token').replace('lowercasing', 'lowercase')\n",
        "\n",
        "print(sentence1)\n",
        "print(sentence2)"
      ],
      "metadata": {
        "id": "CBDFmzPGe8Cy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A computer can't really make sense of sentences (atleast not before I tell you it does). So let's help break the sentences into words, called tokens."
      ],
      "metadata": {
        "id": "VNNU4SKLj6xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "tokens1 = nltk.word_tokenize(sentence1)\n",
        "tokens2 = nltk.word_tokenize(sentence2)\n",
        "\n",
        "print(tokens1)\n",
        "print(tokens2)"
      ],
      "metadata": {
        "id": "yrDfnCcWj6Z2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens1 = [t for t in tokens1 if t.isalpha()]\n",
        "tokens2 = [t for t in tokens2 if t.isalpha()]\n",
        "\n",
        "print(tokens1)\n",
        "print(tokens2)"
      ],
      "metadata": {
        "id": "5v1Q5dPS53Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "another quick question: are there words that are not as important or maybe that aren't useful topics?"
      ],
      "metadata": {
        "id": "6lOoS1RQgZjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# remove all the stop words\n",
        "stop_words = ['i', 'is', 'am', 'are', 'will', 'and', 'be', 'a', 'to', 'of', 'how', 'this']\n",
        "tokens1 = [t for t in tokens1 if t not in stop_words]\n",
        "tokens2 = [t for t in tokens2 if t not in stop_words]\n",
        "\n",
        "print(tokens1)\n",
        "print(tokens2)"
      ],
      "metadata": {
        "id": "c-xEbRKtgLcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different techniques compare these lists to compare similarity between any two documents. Let's apply these techniques to our collected data!"
      ],
      "metadata": {
        "id": "t_MapEMe3H-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cleaning our dataset"
      ],
      "metadata": {
        "id": "9XmptS9holql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "import string\n",
        "import re\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = ' '.join([word for word in text.split() if word not in ENGLISH_STOP_WORDS])  # Remove stopwords\n",
        "    return text\n",
        "\n",
        "# Assuming df_posts is your DataFrame and 'selftext' is the column with text data\n",
        "texts = df_posts['title'] + '. ' + df_posts['selftext']\n",
        "\n",
        "# texts = df_posts['title']\n",
        "# texts = df_posts['selftext']\n",
        "\n",
        "# Apply preprocessing to each document\n",
        "processed_texts = [preprocess(text) for text in texts]\n",
        "\n",
        "# Tokenize the documents\n",
        "tokenized_texts = [text.split() for text in processed_texts]\n",
        "\n",
        "# Create a Gensim dictionary and corpus\n",
        "dictionary = corpora.Dictionary(tokenized_texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
        "print(\"Our code is cleaned\")"
      ],
      "metadata": {
        "id": "mUVSVxJm77r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is how the entries look like after cleaning.\n",
        "for doc_token in tokenized_texts[:10]:\n",
        "  print(doc_token)"
      ],
      "metadata": {
        "id": "rxto7B8Ao-H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Part 3: Can I see some graphs?\n",
        "Extracting topics!\n",
        "Try changing the number of topics and passes to see how the results change."
      ],
      "metadata": {
        "id": "7kitgFy7oxK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LDA model\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, passes=20)\n",
        "\n",
        "# Print the topics\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "metadata": {
        "id": "jOd7fkd07-rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def get_top_n_words(lda_model, dictionary, n_top_words=10):\n",
        "    all_top_words = set()\n",
        "\n",
        "    for topic in lda_model.get_topics():\n",
        "        top_feature_ids = topic.argsort()[-n_top_words:][::-1]\n",
        "        top_words = [dictionary[id] for id in top_feature_ids]\n",
        "        all_top_words.update(top_words)\n",
        "\n",
        "    return all_top_words\n",
        "\n",
        "def plot_top_words(lda_model, dictionary, n_top_words, n_top_topics):\n",
        "\n",
        "    # Get the number of topics\n",
        "    num_topics = len(lda_model.get_topics())\n",
        "\n",
        "    top_n_words = get_top_n_words(lda_model, dictionary, n_top_words=n_top_words)\n",
        "    # Define a color palette with 21 unique colors\n",
        "    palette = [\n",
        "      \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n",
        "      \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n",
        "      \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n",
        "      \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\",\n",
        "      \"#7fc97f\", \"#beaed4\", \"#fdc086\", \"#ffff99\", \"#386cb0\",\n",
        "      \"#f0027f\", \"#bf5b17\", \"#666666\", \"#1b9e77\", \"#d95f02\"\n",
        "  ]\n",
        "\n",
        "    # Set the palette\n",
        "    word_colors = dict(zip(top_n_words, sns.color_palette(palette, len(top_n_words))))\n",
        "\n",
        "    # Create a single figure and multiple subplots (axes) arranged in one row\n",
        "    fig, axes = plt.subplots(1, n_top_topics, figsize=(n_top_topics * 6, 8))\n",
        "\n",
        "    for topic_idx, topic in enumerate(lda_model.get_topics()[:n_top_topics]):\n",
        "        top_feature_ids = topic.argsort()[-n_top_words:][::-1]\n",
        "        top_words = [dictionary[id] for id in top_feature_ids]\n",
        "        weights = topic[top_feature_ids]\n",
        "\n",
        "        # Get consistent colors for words from the color map\n",
        "        current_word_colors = [word_colors[word] for word in top_words]\n",
        "\n",
        "        ax = axes[topic_idx]\n",
        "        ax.barh(top_words, weights, color=current_word_colors)\n",
        "        ax.invert_yaxis()\n",
        "        ax.set_title(f'Topic {topic_idx + 1}', fontsize=24, fontweight='bold', pad=20)\n",
        "        ax.set_xlabel('Word Probability', fontsize=18)\n",
        "        ax.set_ylabel('Words', fontsize=18)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=14)\n",
        "        ax.grid(True, which=\"both\", ls=\"--\", c='0.7')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "piWeiNy5-1Cr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We have a list of extracted topics from our own collected data!\n",
        "Wait... why do they all look the same and why is trymp in most of them? Maybe my model was dumb! Or it wasn't context aware (try and recall the paper you read last week). Language context matters!"
      ],
      "metadata": {
        "id": "rXWC_4Fh7Xyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_top_words(lda_model, dictionary, n_top_words=7, n_top_topics=5)"
      ],
      "metadata": {
        "id": "dOMkQ_Vdp59z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lets try context-aware models to do the same thing and see what topics we get.\n",
        "ahmmm wait. But what does context even mean? </br>\n",
        "\n",
        "The following sentences will have the same tokens. \\[lets, eat, grandpa\\] but the meaning is different. </br>\n",
        "\n",
        " - 'let's eat grandpa'\n",
        " - 'grandpa let's eat'\n",
        "\n",
        "</br>\n",
        "\n",
        "The new machine learning models can somehow capture this! Let's put them to test."
      ],
      "metadata": {
        "id": "DB8mIUXOr2NU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "# Create an instance of BERTopic\n",
        "topic_model = BERTopic(min_topic_size=4)\n",
        "\n",
        "# processed_text\n",
        "\n",
        "# Fit the model on your data to retrieve topics\n",
        "topics = topic_model.fit_transform(text)"
      ],
      "metadata": {
        "id": "TkUtllB1ts9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show me the top 7 topics that you have collected and top 3 words that contribute the most in making up the topic"
      ],
      "metadata": {
        "id": "jtHHcikE88cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_barchart(top_n_topics=4, n_words=10, width=450, height=400)"
      ],
      "metadata": {
        "id": "FQAvvjWex90P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How far away the topics are from each other."
      ],
      "metadata": {
        "id": "5VC4Srpc9EU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics(top_n_topics=6)"
      ],
      "metadata": {
        "id": "ADtWjhYkz4Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Everybody loves word clouds!\n",
        "\n",
        "Let's see which words occur the most in our collected data."
      ],
      "metadata": {
        "id": "vrLK60CTrJZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# join all the posts to make one large paragraph\n",
        "text_data = ' '.join(texts)\n",
        "processed_text = ' '.join(processed_texts)\n",
        "\n",
        "def generate_word_cloud(text):\n",
        "\n",
        "    # Generate the word cloud\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "    # Display the word cloud using matplotlib\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')  # Hide the axes\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "G7omIT_4S8g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_word_cloud(text_data)"
      ],
      "metadata": {
        "id": "sisuUd3KR-oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_word_cloud(processed_text)"
      ],
      "metadata": {
        "id": "K1DsTMjTEz2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What about sentiment analysis?\n",
        "Let's calculate the sentiments and see some example posts ðŸ™‚"
      ],
      "metadata": {
        "id": "wtpjhZMkCaPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "def calculate_sentiment(text):\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    sentiment = sia.polarity_scores(text)['compound']\n",
        "    return sentiment\n",
        "\n",
        "# Calculate sentiment\n",
        "df_posts['sentiment'] = (df_posts['title']+df_posts['selftext']).apply(calculate_sentiment)"
      ],
      "metadata": {
        "id": "onu5dmzGrjVj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "df_posts['sentiment_label'] = pd.cut(\n",
        "    df_posts['sentiment'],\n",
        "    bins=[-1, -0.1, 0.1, 1],\n",
        "    labels=['negative', 'neutral', 'positive']\n",
        ")\n"
      ],
      "metadata": {
        "id": "WQDODJrMCeyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='sentiment_label', data=df_posts, palette='viridis')\n",
        "plt.title('Sentiment Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jAJurmo0C9-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y4Dpm_cmDW0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_posts"
      ],
      "metadata": {
        "id": "whzV48NqGCKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the count of each sentiment label\n",
        "sentiment_counts = df_posts['sentiment_label'].value_counts()\n",
        "\n",
        "# Calculate the total count of sentiments\n",
        "total_sentiments = len(df_posts)\n",
        "\n",
        "# Calculate the percentage of each sentiment label\n",
        "sentiment_percentage = (sentiment_counts / total_sentiments) * 100\n",
        "\n",
        "# Display the percentage of each sentiment label\n",
        "for sentiment, percentage in sentiment_percentage.items():\n",
        "    print(f\"The percentage of {sentiment} sentiments is {percentage:.2f}%\")\n",
        "\n",
        "# Set up the seaborn style and color palette\n",
        "sns.set_style(\"whitegrid\")\n",
        "palette = sns.color_palette(\"viridis\", n_colors=sentiment_counts.shape[0])\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.barplot(x=sentiment_percentage.index, y=sentiment_percentage.values, palette=palette)\n",
        "\n",
        "# Title and labels\n",
        "plt.title('Percentage Distribution of Sentiments', fontsize=20, fontweight='bold', pad=20)\n",
        "plt.xlabel('Sentiment', fontsize=16, labelpad=10)\n",
        "plt.ylabel('Percentage (%)', fontsize=16, labelpad=10)\n",
        "\n",
        "# Beautify the axes and grid\n",
        "plt.xticks(fontsize=12, rotation=45)\n",
        "plt.yticks(fontsize=12)\n",
        "sns.despine(left=True, bottom=True)  # Remove left and bottom spines\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2B2gDaCXC-OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49k7_FWCqohw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}